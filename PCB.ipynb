{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking BP1 as an example (two variables: H10 and H100)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#  Input data (the sample plots from transfer area)\n",
    "data = pd.read_excel(\"./trainSMGFfujianCOM.xlsx\", header=None)\n",
    "data_np = data.to_numpy()\n",
    "\n",
    "#  Parameter Constraint Ranges (Two Variables)\n",
    "param_bounds = {\n",
    "    \"weight_0\": (-1.4234, -0.0906),   # H10  The numerical range depends on the parameter distribution of the model derived from your existing dataset.\n",
    "    \"weight_1\": (4.98, 7.557),  # H100 \n",
    "}\n",
    "\n",
    "k = len(data_np)  #Sample size\n",
    "\n",
    "# Dynamic Constraint Coefficient Function\n",
    "def get_lambda_constraint(k, current_mse, max_lambda=30):\n",
    "    if k < 5:\n",
    "        return min(0.5 * current_mse, max_lambda)\n",
    "    elif k <= 15:\n",
    "        scale = (15 - k) / 10\n",
    "        return min(0.5 * current_mse * scale, max_lambda)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Constraint Loss Function (2 weights)\n",
    "def constraint_loss(model, lambda_constraint):\n",
    "    reg_loss = 0\n",
    "    w = model.weight[0]\n",
    "    reg_loss += (torch.relu(w[0] - param_bounds[\"weight_0\"][1]) + torch.relu(param_bounds[\"weight_0\"][0] - w[0])) ** 2\n",
    "    reg_loss += (torch.relu(w[1] - param_bounds[\"weight_1\"][1]) + torch.relu(param_bounds[\"weight_1\"][0] - w[1])) ** 2\n",
    "    return lambda_constraint * reg_loss\n",
    "\n",
    "# Build the Model\n",
    "model = nn.Linear(2, 1)\n",
    "model.weight = nn.Parameter(torch.tensor([[-0.757, 6.27]]))  # Initial parameter values of the baseline model\n",
    "model.bias = nn.Parameter(torch.tensor([7.283]))  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X = data_np[:, :2]        \n",
    "y = data_np[:, 3]      \n",
    "inputData = torch.from_numpy(X).float()\n",
    "targetData = torch.from_numpy(y).float().view(-1, 1)\n",
    "\n",
    "# Initial MSE and Î»\n",
    "with torch.no_grad():\n",
    "    base_pred = model(inputData)\n",
    "    base_mse = criterion(base_pred, targetData).item()\n",
    "lambda_constraint = get_lambda_constraint(k, base_mse)\n",
    "\n",
    "# MOdel training\n",
    "model.train()\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(inputData)\n",
    "\n",
    "    loss = criterion(pred, targetData)\n",
    "    loss += constraint_loss(model, lambda_constraint)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# Output Model Parameters\n",
    "final_weights = model.weight.detach().numpy().flatten()\n",
    "final_bias = model.bias.detach().item()\n",
    "print(\"Training completed. Model parameters are as follows:\")\n",
    "print(f\"H10 = {final_weights[0]:.4f}\")\n",
    "print(f\"H100 = {final_weights[1]:.4f}\")\n",
    "print(f\"bias = {final_bias:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
